{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-voxel pattern analysis\n",
    "\n",
    "This week we will use a popular python library to do simple classification of a fake data set, and of some real fMRI data. My original intent was to implement a linear discriminant from scratch, but... that's a bit more math than we have time for in this session. So instead, we will explore [`sklearn`](http://scikit-learn.org/stable/) a bit, and show you how to make use of python libraries to solve your problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nibabel\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "# For to fancy\n",
    "import fakedata as fd\n",
    "import cortex as cx\n",
    "\n",
    "# For to machine learn\n",
    "from sklearn import svm\n",
    "from sklearn import discriminant_analysis as da\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple classifier with fake data\n",
    "First, to perform classification at all, we need some data. We need an `X` variable, consisting of the features to use for classification, and a `Y` variable, consisting of class labels. This should be pretty easy to generate, given what we have learned so far in class. Your first task is to write code to generate the plot below!\n",
    "\n",
    "<img src=\"TwoClasses.png\">\n",
    "\n",
    "You should write your code in such a way that it is easy to make the distributions of points closer to each other / slightly overlapping. Also, you should divide your data into a training / testing set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 2D values for class 1\n",
    "n = 500 # number of exemplars per class\n",
    "#r = 0.6\n",
    "mu_c1 = np.array([-0.5, 0.3])\n",
    "mu_c2 = np.array([0.5, -0.3])\n",
    "Xt, Xv, Yt, Yv = generate_data(n, mu_c1, mu_c2) #, cov_c1=r, cov_c2=r)\n",
    "\n",
    "plot_classes(Xt[Yt==1], Xt[Yt==2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn!\n",
    "\n",
    "Now we will use the sklearn (scikit-learn) machine learning library to classify the points into each class! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get SVM classifier base object\n",
    "svmclf = svm.LinearSVC()\n",
    "# Fit SVM classifier to data\n",
    "_ = svmclf.fit(Xt, Yt)\n",
    "# Predict new values\n",
    "Ypred_svm = svmclf.predict(Xv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldaclf = da.LinearDiscriminantAnalysis()\n",
    "# Fit LDA classifier to data\n",
    "_ = ldaclf.fit(Xt, Yt)\n",
    "# Predict new responses w/ LDA classifier\n",
    "Ypred_lda = ldaclf.predict(Xv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How should we test accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show decision boundary \n",
    "### But first, a little basic algebra review...\n",
    "\n",
    "Perhaps the most familiar form of a line is:\n",
    "\n",
    "$y = mx + b$\n",
    "\n",
    "...but a more general form for a line is the following: (this can take x, y, z,... arbitrarily many variables, each with a constant). \n",
    "\n",
    "$0 = Ax + By + C$\n",
    "\n",
    "This form of a line is what is returned by the classification algorithms. To put this into the more familiar form (for a 2D line only), we can do the following:\n",
    "\n",
    "$m = -A/B$ \n",
    "\n",
    "and\n",
    "\n",
    "$b = -C/B$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ax + By + C form:\n",
    "print(ldaclf.coef_) # [A, B]\n",
    "print(ldaclf.intercept_) # [C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show slope & intercept of decision plane line for lda\n",
    "print('Slope = %0.2f'%(- ldaclf.coef_[0][0]/ldaclf.coef_[0][1]))\n",
    "print('Intercept = %0.2f'%(- ldaclf.intercept_[0]/ldaclf.coef_[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show slope & intercept of decision plane line for svm\n",
    "print('Slope = %0.2f'%(- svmclf.coef_[0][0]/svmclf.coef_[0][1]))\n",
    "print('Intercept = %0.2f'%(- svmclf.intercept_[0]/svmclf.coef_[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_classes(Xt[Yt==1], Xt[Yt==2], classifier=svmclf, clfcolor='c-', ax=ax)\n",
    "plot_classes(Xt[Yt==1], Xt[Yt==2], classifier=ldaclf, clfcolor='m-', plot_data=False, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_classes(Xv[Yv==1], Xv[Yv==2], classifier=svmclf, clfcolor='c-', ax=ax)\n",
    "plot_classes(Xv[Yv==1], Xv[Yv==2], classifier=ldaclf, clfcolor='m-', plot_data=False, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How would you change the data to make the classifier more or less accurate? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MVPA on real fMRI data\n",
    "Now, let's try this on something more interesting - actual fMRI data. We will analyze the data from a simple localizer experiment, and try to classify which type of stimulus is present for each TR of a withheld run. \n",
    "\n",
    "First, we have to define our `X` and `Y` variables. NOTE that here, X is voxels! Up to now, Y has been voxel activity, but in this analysis, the direction of fitting is backwards with respect to regression - we are using *voxel activity* to predict *stimulus class* rather than stimulus class (or stimulus features, or experimental condition) to predict voxel activity.\n",
    "\n",
    "Thus, X will be voxels. Our first decision is WHICH voxels to include. We will NOT do a full-brain analysis, here - we will select a large region of interest from combining pre-existing ROIs, using pycortex. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 's03'\n",
    "transform = 'category_localizer'\n",
    "roi_masks = cx.get_roi_masks('s03', 'category_localizer', roi_list=['V1','V2','V3','V4','LO','OFA','FFA','EBA',\n",
    "                                                                   'PPA','RSC','OPA'])\n",
    "all_masks = np.array(list(roi_masks.values()))\n",
    "print(all_masks.shape)\n",
    "mask = np.any(all_masks, axis=0)\n",
    "print(mask.shape)\n",
    "#cx.webgl.show(cx.Volume(mask, subject, transform))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Show the mask using pycortex! Which voxels are selected? Did we do this right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will load 6 runs of localizer data, masking each run with the mask we have created to select only the voxels we care about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdir = '/unrshare/LESCROARTSHARE/IntroToEncodingModels/'\n",
    "fbase = os.path.join(fdir, 's03_catloc_run%02d.nii.gz')\n",
    "data = []\n",
    "for run in range(1, 7):\n",
    "    nii = nibabel.load(fbase%run)\n",
    "    # Transpose at load time to make the data [t, z, y, x]\n",
    "    tmp = nii.get_data().T\n",
    "    # Mask the data to select only the voxels we care about\n",
    "    data.append(tmp[:, mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How would you create training and testing X data from this list of arrays?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need Y, the labels for classes. This will come from the design matrix of the localizer experiment (formerly known as X,for regression!). Each class will be the type of stimulus shown in each block of the experiment (faces, places, bodies, etc). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(os.path.join(fdir, 'catloc_design.hdf')) as hf:\n",
    "    print(list(hf.keys()))\n",
    "    X = hf['X'].value\n",
    "    xnames = hf['xnames'].value.tolist()\n",
    "    # Ignore the 'decode' for now, it has to do with the format in which the strings were stored in this \n",
    "    # file, and it's just confusing...\n",
    "    class_names = ['null'] + [x.decode() for x in xnames]\n",
    "    events = hf['events'].value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What are these variables? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "# (Display each variable, figure out what each is!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need Y variables. Which of these variables should be Y? \n",
    "> Define Y (The classes of the stimuli!) separately for training (Yt) and testing or validation (Yv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now, classify away! (Use the same sklearn classifiers we used above to try to classify the testing stimuli. Check how accurate your answers are!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn gives you a nice way to make confusion matrices, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmatrix = metrics.confusion_matrix(Yv, Yv_pred)\n",
    "\n",
    "# Show matrix\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cmatrix)\n",
    "ax.set_xticks(np.arange(6))\n",
    "ax.set_yticks(np.arange(6))\n",
    "ax.set_xticklabels(class_names, rotation=90)\n",
    "ax.set_yticklabels(class_names)\n",
    "plt.colorbar(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's show the classifier weights in pycortex!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Where are the weights stored? \n",
    "\n",
    "(Remember that we are doing one-vs-all classifications, so there will be different weights to classify each class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How might you make your classification accuracy better? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
