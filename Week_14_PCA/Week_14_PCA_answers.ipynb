{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This week: PCA! \n",
    "\n",
    "Last week, we ran into a fundamental difference between encoding models and pRF-style fits: that with encoding models, you often end up with a great many beta weights for many potentially correlated model features. This week, we will discuss one approach to interpreting many many beta weights, i.e. doing dimensionality reduction with PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys # library for changing system-level settings\n",
    "import os # library for navigating the operating system, particularly useful for file paths\n",
    "import json # library to load and save dictionary-like files \n",
    "            # NOTE: good for transfer of struct arrays / dicts from matlab->python or vice versa\n",
    "import h5py # For loading example fMRI data from saved files\n",
    "\n",
    "# pycortex\n",
    "import cortex as cx\n",
    "# PCA function\n",
    "from sklearn.decomposition import PCA\n",
    "# Z-score function\n",
    "from scipy.stats import zscore\n",
    "# Local utility functions\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "import utils\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA finds a set of dimension (basis vectors) onto which data can be projected. So, first, let's play with the linear algebra concept of *projection*.\n",
    "\n",
    "# Projection\n",
    "First: Projection is multiplication. You should not be afraid of the term. It's another concept with a simple 2D analogy that you have to kind of stretch your brain to think about in multiple dimensions. Here, we will demonstrate projection of one vector onto another in 2D. \n",
    "\n",
    "First, we'll define two vectors, a and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two 2D vectors\n",
    "a = np.array([2, 7])\n",
    "b = np.array([3, 4])\n",
    "# Plot them!\n",
    "plt.plot([0, a[0]], [0, a[1]])\n",
    "plt.plot([0, b[0]], [0, b[1]])\n",
    "# Label them!\n",
    "plt.text(a[0], a[1], 'a', fontsize=16)\n",
    "plt.text(b[0], b[1], 'b', fontsize=16)\n",
    "# Equal axes make all the following plots nicer\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One preliminary concept is, how long is `a`? and how long is `b`? \n",
    "\n",
    "In 2D, this is easy - back to Pythagoras:\n",
    "\n",
    "### $a^2 + b^2 = c^2$\n",
    "\n",
    "...so:\n",
    "\n",
    "### $c = \\sqrt{a^2 + b^2}$\n",
    "\n",
    "This holds in many dimensions, too - so the length of a vector is always:\n",
    "\n",
    "### $ \\displaystyle{L = \\sqrt{ \\Sigma_{i=0}^nx_i^2}}$\n",
    "\n",
    "and sure enough, there is a numpy function for this: `np.linalg.norm` computes the length of a vector, or the *vector norm*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_a = np.linalg.norm(a)\n",
    "length_b = np.linalg.norm(b)\n",
    "print(length_a)\n",
    "print(length_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute projection of b onto a:\n",
    "proj_b_to_a = np.sum(b * a) / length_a\n",
    "print(proj_b_to_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to do this is to do a matrix multiplication between the two. For that to work, each has to be 2D.\n",
    "\n",
    "For this simple demo, we will stick with the variables above, but the matrix form of this (`<array>.T.dot(<array>)`) will be useful when we want to do projections of more vectors in more dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape a and b to have a (one-unit long) second dimension, so we can transpose them\n",
    "a_2d = a.reshape(2, 1)\n",
    "b_2d = b.reshape(2, 1)\n",
    "# Matrix multiply & divide by norm of a\n",
    "proj_b_to_a_fancy = a_2d.T.dot(b_2d) / length_a\n",
    "# Same answer as above...\n",
    "print(proj_b_to_a_fancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the vectors again\n",
    "plt.plot([0, a[0]], [0, a[1]])\n",
    "plt.plot([0, b[0]], [0, b[1]])\n",
    "# Label them!\n",
    "plt.text(a[0], a[1], 'a', fontsize=16)\n",
    "plt.text(b[0], b[1], 'b', fontsize=16)\n",
    "# And plot the point at which one projects onto the other\n",
    "frac_of_a = proj_b_to_a / length_a\n",
    "c1 = a[0]*frac_of_a\n",
    "c2 = a[1]*frac_of_a\n",
    "plt.plot([0, c1], [0, c2], 'r.--')\n",
    "plt.plot([c1, b[0]], [c2, b[1]], 'r.--')\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of the projection as the \"shadow\" that b casts on a (when the sun is perpendicular to a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Vary the locations of a and b, and see what happens to the projection. As an exercise in plotting, see what happens when you remove the plt.axis('equal') from the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE a and b!\n",
    "a = np.array([2, 7])\n",
    "b = np.array([3, 4])\n",
    "\n",
    "# Plot the vectors\n",
    "plt.plot([0, a[0]], [0, a[1]])\n",
    "plt.plot([0, b[0]], [0, b[1]])\n",
    "# Label the vectors\n",
    "plt.text(a[0], a[1], 'a', fontsize=16)\n",
    "plt.text(b[0], b[1], 'b', fontsize=16)\n",
    "# Compute projection\n",
    "proj_b_to_a = np.sum(b * a) / np.linalg.norm(a)\n",
    "# And plot the point at which one projects onto the other\n",
    "frac_of_a = proj_b_to_a / length_a\n",
    "c1 = a[0]*frac_of_a\n",
    "c2 = a[1]*frac_of_a\n",
    "plt.plot([0, c1], [0, c2], 'r.--')\n",
    "plt.plot([c1, b[0]], [c2, b[1]], 'r.--')\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Is the projection of b onto a the same as the distance from b to a? Is it the same as the angle between b and a? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data creation \n",
    "if you're curious how to create arrays with particular covariance structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # This has already been done, don't overwrite saved data\n",
    "    n_voxels = 335\n",
    "    n_features = 44\n",
    "    u, s, vt = np.linalg.svd(np.random.randn(n_voxels, n_features))\n",
    "\n",
    "    n = 10\n",
    "    x = np.linspace(0, 1, n_features)\n",
    "    w = np.exp(-x**2/ (2*0.05**2))\n",
    "    plt.plot(x[:n], w[:n], '.-')\n",
    "\n",
    "    m, n = n_voxels, n_features\n",
    "    Sd = np.diag(s*w)\n",
    "    Sd = np.pad(Sd,[(0,m-n),(0,0)],mode='constant')\n",
    "    y = u.dot(Sd).dot(vt.T)\n",
    "\n",
    "    plt.imshow(y, aspect='auto')\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.ylabel(\"Voxels\")\n",
    "\n",
    "    np.save('pca_data.npy', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing PCA\n",
    "... with `sklearn` again! The implemetation of this is super simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "y = np.load('pca_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PCA fitting object from the PCA object in sklearn (imported above)\n",
    "pca_fake = PCA(whiten=True)\n",
    "# Fit the PCA algorithm to the data!\n",
    "pca_fake.fit(y);\n",
    "# (technically, doing this on y and the covariance matrix of y is approximately the same thing...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components quantify covariance of FEATURES across VOXELS.\n",
    "print(pca_fake.components_.shape)\n",
    "# Each ROW of this is a component!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see how much variance each component explains, make a scree plot using the explained_variance_ratio_ field\n",
    "# (as with all objects in sklearn, properties of the fit object with \"_\" at the end are estimated quantities)\n",
    "plt.plot(pca_fake.explained_variance_ratio_[:10], 's-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show what PCA is doing, let's have a look at the covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the covariance of y across voxels (so the end result is features x features)\n",
    "y_demean = y - y.mean(0) # subtract off the mean of each column (axis=0 -> mean over columns)\n",
    "ycov = y_demean.T.dot(y_demean) # dot product\n",
    "ycov /= (len(y)-1) # normalize by number of elements - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show what the covariance across voxels\n",
    "plt.imshow(ycov)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is an example of how to reconstruct the features x features covariance matrix using only ONE principal component.\n",
    "\n",
    "> Reconstruct the covariance matrix with > 1 principal component! See how close the the result ends up looking to the real covariance matrix above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(12, 3))\n",
    "for nc, ax in enumerate(axs, 1):\n",
    "    pca_cov = (pca_fake.explained_variance_[:nc, np.newaxis] * pca_fake.components_[:nc]).T.dot(pca_fake.components_[:nc])\n",
    "    im = ax.imshow(pca_cov, vmin=-0.25, vmax=0.35)\n",
    "    ax.set_title('Reconstructed with\\n%d components'%nc)\n",
    "#fig.colorbar(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute how much variance this PC explains (this is approximate...)\n",
    "1 - np.var(ycov-pca_cov) / np.var(ycov, ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With real fMRI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Y variables (fMRI data for 1260 estimation images, 126 validation images)\n",
    "with h5py.File('/unrshare/LESCROARTSHARE/IntroToEncodingModels/s01_color_natims_data.hdf') as hf:\n",
    "    Y_est = hf['est'].value\n",
    "    Y_val = hf['val'].value\n",
    "    mask = hf['mask'].value   \n",
    "    \n",
    "# Load X variable (Semantic category features)\n",
    "with h5py.File('/unrshare/LESCROARTSHARE/IntroToEncodingModels/color_natims_features_19cat.hdf') as hf:\n",
    "    X_est = hf['est'].value\n",
    "    X_val = hf['val'].value\n",
    "\n",
    "print(Y_est.shape)\n",
    "print(X_est.shape)\n",
    "print(Y_val.shape)\n",
    "print(X_val.shape)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute regression to estimate weights\n",
    "B = utils.ols(X_est, Y_est)\n",
    "print(B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate predictions\n",
    "Y_hat = X_val.dot(B)\n",
    "# Compute prediction accuracy (correlation btw Y_val and Y_hat)\n",
    "r = utils.column_corr(Y_val, Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of NaNs (histograms don't like nans)\n",
    "r_nonans = r[~np.isnan(r)] # (this creates a logical index (~np.isnan(r)) and indexes r with it)\n",
    "plt.hist(r_nonans, bins=100)\n",
    "plt.xlabel(\"Prediction accuracy (r)\")\n",
    "plt.ylabel(\"Voxels (count)\")\n",
    "_ = plt.annotate('Check out\\nthis tail!', (0.5, 200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where did we predict well? \n",
    "i.e., where are those voxels in the tail of the distribution for which we have decent predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create volume for correlation coefficient, visualize across the brain!\n",
    "subject = 's01'\n",
    "transform = 'color_natims'\n",
    "Vr = cx.Volume(r, subject, transform, mask=mask, cmap='inferno', vmin=0, vmax=0.8)\n",
    "fig = cx.quickflat.make_figure(Vr, with_curvature=True, with_sulci=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay, we can predict semantic-y / visual category-y areas! Let's do PCA across only those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voxel selection\n",
    "For PCA, we want to perform some voxel selection - let's pick only the voxels that we can predict better than r = 0.2\n",
    "\n",
    "> DO IT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Answer\n",
    "# select only voxels with prediction accuracy greater than 0.2\n",
    "good_voxels = r > 0.2 # Create a logical index for voxel dimension\n",
    "B_forpca = B[:, good_voxels] # apply it to the voxel dimension of B\n",
    "print(B_forpca.shape) # End up with far fewer voxels!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit PCA for voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PCA object\n",
    "voxel_pca = PCA(n_components=3)\n",
    "# Transpose Beta weights to compute PCA across features, not across voxels\n",
    "# (PCA will collapse across the first dimension of the array - here, we want that to be voxels, \n",
    "# in order to find common patterns of feature weights across voxels)\n",
    "voxel_pca.fit(B_forpca.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the PC across semantic categories\n",
    "NOTE: we didn't do this in class, but this is useful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the names for each semantic feature\n",
    "sem_feat_names = json.load(open('/unrshare/LESCROARTSHARE/IntroToEncodingModels/color_natims_features_19cat.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first PC, sorted by magnitude\n",
    "fig, ax = plt.subplots(figsize=(12,3))\n",
    "# Select the first PC\n",
    "pc1 = voxel_pca.components_[0]\n",
    "# Get an index for which values in the PC are smallest -> largest\n",
    "pc1_idx = np.argsort(pc1)\n",
    "# Plot the PC, sorted by this index\n",
    "plt.plot(pc1[pc1_idx])\n",
    "# Set the ticks to the feature names, sorted by the same index\n",
    "plt.xticks(np.arange(19))\n",
    "ax.set_xticklabels(np.array(sem_feat_names)[pc1_idx], rotation=90, fontsize=16);\n",
    "# Add zero line, for reference\n",
    "plt.axhline(linestyle='--', color='k', lw=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project all voxel weights onto the first PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel_pca.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of NaNs (results of dividing by zero for some voxels, which mess things up)\n",
    "B_nonan = np.nan_to_num(B)\n",
    "Bt = voxel_pca.transform(B_nonan.T)\n",
    "# NOTE: this is equivalent to doing this:\n",
    "B_demean = B_nonan - voxel_pca.mean_[:, np.newaxis] # extra step of removing mean of voxel weights \n",
    "Bt_alt = voxel_pca.components_.dot(B_demean)\n",
    "print(np.allclose(Bt.T, Bt_alt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show projections of all voxels onto first PC\n",
    "Vpc1 = cx.Volume(Bt[:,0], subject, transform, cmap='BuWtRd', vmin=-0.4, vmax=0.4)\n",
    "fig = cx.quickflat.make_figure(Vpc1, with_curvature=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pycortex advertisement (slightly fancier plot w/ voxels w/ poor predictions alpha-d out)\n",
    "Vpc1a = cx.Volume2D(Bt[:,0], r, subject, transform, cmap='BuBkRd_alpha_2D', vmin=-0.4, vmax=0.4,\n",
    "                   vmin2=0.2, vmax2=0.8)\n",
    "fig = cx.quickflat.make_figure(Vpc1a, with_curvature=True, with_colorbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is approximately the plot Figure 8 (for S2) from Naselaris et al 2012; it likely differes slightly due to different voxel selection (and colormap choice). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
