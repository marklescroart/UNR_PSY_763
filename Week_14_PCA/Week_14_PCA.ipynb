{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This week: PCA! \n",
    "\n",
    "Last week, we ran into a fundamental difference between encoding models and pRF-style fits: that with encoding models, you often end up with a great many beta weights for many potentially correlated model features. This week, we will discuss one approach to interpreting many many beta weights, i.e. doing dimensionality reduction with PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys # library for changing system-level settings\n",
    "import os # library for navigating the operating system, particularly useful for file paths\n",
    "import json # library to load and save dictionary-like files \n",
    "            # NOTE: good for transfer of struct arrays / dicts from matlab->python or vice versa\n",
    "\n",
    "# pycortex\n",
    "import cortex as cx\n",
    "# PCA function\n",
    "from sklearn.decomposition import PCA\n",
    "# Z-score function\n",
    "from scipy.stats import zscore\n",
    "# Local utility functions\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "import utils\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA finds a set of dimension (basis vectors) onto which data can be projected. So, first, let's play with the linear algebra concept of *projection*.\n",
    "\n",
    "# Projection\n",
    "First: Projection is multiplication. You should not be afraid of the term. It's another concept with a simple 2D analogy that you have to kind of stretch your brain to think about in multiple dimensions. Here, we will demonstrate projection of one vector onto another in 2D. \n",
    "\n",
    "First, we'll define two vectors, a and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two 2D vectors\n",
    "a = np.array([2, 7])\n",
    "b = np.array([3, 4])\n",
    "# Plot them!\n",
    "plt.plot([0, a[0]], [0, a[1]])\n",
    "plt.plot([0, b[0]], [0, b[1]])\n",
    "# Label them!\n",
    "plt.text(a[0], a[1], 'a', fontsize=16)\n",
    "plt.text(b[0], b[1], 'b', fontsize=16)\n",
    "# Equal axes make all the following plots nicer\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One preliminary concept is, how long is `a`? and how long is `b`? \n",
    "\n",
    "In 2D, this is easy - back to Pythagoras:\n",
    "\n",
    "### $a^2 + b^2 = c^2$\n",
    "\n",
    "...so:\n",
    "\n",
    "### $c = \\sqrt{a^2 + b^2}$\n",
    "\n",
    "This holds in many dimensions, too - so the length of a vector is always:\n",
    "\n",
    "### $ \\displaystyle{L = \\sqrt{ \\Sigma_{i=0}^nx_i^2}}$\n",
    "\n",
    "and sure enough, there is a numpy function for this: `np.linalg.norm` computes the length of a vector, or the *vector norm*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_a = np.linalg.norm(a)\n",
    "length_b = np.linalg.norm(b)\n",
    "print(length_a)\n",
    "print(length_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute projection of b onto a:\n",
    "proj_b_to_a = np.sum(b * a) / length_a\n",
    "print(proj_b_to_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to do this is to do a matrix multiplication between the two. For that to work, each has to be 2D.\n",
    "\n",
    "For this simple demo, we will stick with the variables above, but the matrix form of this (`<array>.T.dot(<array>)`) will be useful when we want to do projections of more vectors in more dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape a and b to have a (one-unit long) second dimension, so we can transpose them\n",
    "a_2d = a.reshape(2, 1)\n",
    "b_2d = b.reshape(2, 1)\n",
    "# Matrix multiply & divide by norm of a\n",
    "proj_b_to_a_fancy = a_2d.T.dot(b_2d) / length_a\n",
    "# Same answer as above...\n",
    "print(proj_b_to_a_fancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the vectors again\n",
    "plt.plot([0, a[0]], [0, a[1]])\n",
    "plt.plot([0, b[0]], [0, b[1]])\n",
    "# Label them!\n",
    "plt.text(a[0], a[1], 'a', fontsize=16)\n",
    "plt.text(b[0], b[1], 'b', fontsize=16)\n",
    "# And plot the point at which one projects onto the other\n",
    "frac_of_a = proj_b_to_a / length_a\n",
    "c1 = a[0]*frac_of_a\n",
    "c2 = a[1]*frac_of_a\n",
    "plt.plot([0, c1], [0, c2], 'r.--')\n",
    "plt.plot([c1, b[0]], [c2, b[1]], 'r.--')\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of the projection as the \"shadow\" that b casts on a (when the sun is perpendicular to a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Vary the locations of a and b, and see what happens to the projection. As an exercise in plotting, see what happens when you remove the plt.axis('equal') from the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE a and b!\n",
    "a = np.array([2, 7])\n",
    "b = np.array([3, 4])\n",
    "\n",
    "# Plot the vectors\n",
    "plt.plot([0, a[0]], [0, a[1]])\n",
    "plt.plot([0, b[0]], [0, b[1]])\n",
    "# Label the vectors\n",
    "plt.text(a[0], a[1], 'a', fontsize=16)\n",
    "plt.text(b[0], b[1], 'b', fontsize=16)\n",
    "# Compute projection\n",
    "proj_b_to_a = np.sum(b * a) / length_a\n",
    "# And plot the point at which one projects onto the other\n",
    "frac_of_a = proj_b_to_a / length_a\n",
    "c1 = a[0]*frac_of_a\n",
    "c2 = a[1]*frac_of_a\n",
    "plt.plot([0, c1], [0, c2], 'r.--')\n",
    "plt.plot([c1, b[0]], [c2, b[1]], 'r.--')\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Is the projection of b onto a the same as the distance from b to a? Is it the same as the angle between b and a? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data creation (if you're curious how to create arrays with particular covariance structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_voxels = 335\n",
    "n_features = 44\n",
    "u, s, vt = np.linalg.svd(np.random.randn(n_voxels, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "x = np.linspace(0, 1, n_features)\n",
    "w = np.exp(-x**2/ (2*0.05**2))\n",
    "plt.plot(x[:n], w[:n], '.-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = n_voxels, n_features\n",
    "Sd = np.diag(s*w)\n",
    "Sd = np.pad(Sd,[(0,m-n),(0,0)],mode='constant')\n",
    "y = u.dot(Sd).dot(vt.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(y, aspect='auto')\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Voxels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('pca_data.npy', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing PCA\n",
    "... with `sklearn` again! The implemetation of this is super simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "y = np.load('pca_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PCA fitting object from the PCA object in sklearn (imported above)\n",
    "pca_fake = PCA(whiten=True)\n",
    "# Fit the PCA algorithm to the data!\n",
    "pca_fake.fit(y);\n",
    "# (technically, doing this on y and the covariance matrix of y is approximately the same thing...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components quantify covariance of FEATURES across VOXELS.\n",
    "print(pca_fake.components_.shape)\n",
    "# Each ROW of this is a component!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see how much variance each component explains, make a scree plot using the explained_variance_ratio_ field\n",
    "# (as with all objects in sklearn, properties of the fit object with \"_\" at the end are estimated quantities)\n",
    "plt.plot(pca_fake.explained_variance_ratio_[:10], 's-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show what PCA is doing, let's have a look at the covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the covariance of y across voxels (so the end result is features x features)\n",
    "y_demean = y - y.mean(0) # subtract off the mean of each column (axis=0 -> mean over columns)\n",
    "ycov = y_demean.T.dot(y_demean) # dot product\n",
    "ycov /= (len(y)-1) # normalize by number of elements - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show what the covariance across voxels\n",
    "plt.imshow(ycov)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is an example of how to reconstruct the features x features covariance matrix using only ONE principal component.\n",
    "\n",
    "> Reconstruct the covariance matrix with > 1 principal component! See how close the the result ends up looking to the real covariance matrix above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_fake.explained_variance_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_fake.components_[:n].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_cov = (pca_fake.explained_variance_[:1, np.newaxis] * pca_fake.components_[:1]).T.dot(pca_fake.components_[:1])\n",
    "plt.imshow(pca_cov)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute how much variance this PC explains (this is approximate...)\n",
    "1 - np.var(ycov-pca_cov) / np.var(ycov, ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With real fMRI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Y variables (fMRI data for 1260 estimation images, 126 validation images)\n",
    "with h5py.File('/unrshare/LESCROARTSHARE/IntroToEncodingModels/s01_color_natims_data.hdf') as hf:\n",
    "    Y_est = hf['est'].value\n",
    "    Y_val = hf['val'].value\n",
    "    mask = hf['mask'].value   \n",
    "    \n",
    "# Load X variable (Semantic category features)\n",
    "with h5py.File('/unrshare/LESCROARTSHARE/IntroToEncodingModels/color_natims_features_19cat.hdf') as hf:\n",
    "    X_est = hf['est'].value\n",
    "    X_val = hf['val'].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_est.shape)\n",
    "print(X_est.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate betas!\n",
    "B = ols(X_est, Y_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Use the Betas to predict validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "# ... \n",
    "# ...\n",
    "r = # ...? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where did we predict well? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create volume for correlation coefficient, visualize across the brain!\n",
    "subject = 's01'\n",
    "transform = 'color_natims'\n",
    "Vr = cx.Volume(r, subject, transform, mask=mask, cmap='inferno', vmin=0, vmax=0.8)\n",
    "fig = cx.quickflat.make_figure(Vr, with_curvature=True, with_sulci=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay, we can predict semantic-y / visual category-y areas! Let's do PCA across only those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For PCA, we want to perform some voxel selection - let's pick only the voxels that we can predict better than r = 0.2\n",
    "\n",
    "> DO IT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit PCA for voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel_pca = PCA(n_components=3)\n",
    "voxel_pca.fit(B_forpca.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the PC across semantic categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc1 = voxel_pca.components_[0]\n",
    "pc1_idx = np.argsort(pc1)\n",
    "plt.plot(pc1[pc1_idx])\n",
    "plt.axhline(linestyle='--', color='k', lw=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bt = voxel_pca.transform(np.nan_to_num(B.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vpc1 = cx.Volume(Bt[:,0], subject, transform, cmap='BuWtRd', vmin=-0.4, vmax=0.4)\n",
    "fig = cx.quickflat.make_figure(Vpc1, with_curvature=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pycortex advertisement\n",
    "Vpc1a = cx.Volume2D(Bt[:,0], r, subject, transform, cmap='BuBkRd_alpha_2D', vmin=-0.4, vmax=0.4,\n",
    "                   vmin2=0.2, vmax2=0.8)\n",
    "fig = cx.quickflat.make_figure(Vpc1a, with_curvature=True, with_colorbar=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
