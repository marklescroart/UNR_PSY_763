{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data & Creating X and Y\n",
    "This notebook contains examples of how to:\n",
    "* Create X from stored XLS file info\n",
    "* Create Y from stored variables\n",
    "* Overlay results on makeshift brain\n",
    "* Create an ad hoc ROI (which you will need for MVPA / RSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import h5py\n",
    "import pandas\n",
    "\n",
    "# Add directory above to python path so we can find utility functions\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "# Import local utility functions\n",
    "import utils\n",
    "\n",
    "from scipy.stats import zscore\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: to import the utils functions in another notebook (which you will need), you have to make sure you add "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdir = '/unrshare/LESCROARTSHARE/data_PSY763/SnowLabData/'\n",
    "glob.glob(fdir + '*xls*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load two excel files as data frames in pandas\n",
    "df1 = pandas.read_excel(os.path.join(fdir, 'Sub03_Run_Breakdown.xls'))\n",
    "df2 = pandas.read_excel(os.path.join(fdir, 'Subject 3 Runs with specific object info.xlsx'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasoning through how to extract the relevant info\n",
    "The next sequence of cells show my thought process in reasoning about how to get the relevant info out of these excel files. All the code in them is not strictly necessary; there is a more abbreviated version below. But I thought this would be helpful for you to see how to plod through this rather than jump you to the final answer. There are lots of print statements (showing the size of arrays, showing how many True values in logical indices that are created), which are meant to serve as sanity checks. \n",
    "\n",
    "Here we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, there are 127 * 7 = 889 time points (TRs). Thus, the X for this experiment must be 889 time points long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract a few useful variables\n",
    "# For the first xls file\n",
    "run = df1['Run'].values\n",
    "onset = df1['BV Start'].values\n",
    "offset = df1['BV Stop'].values\n",
    "cat = df1['Run6'].values\n",
    "# Separate indices for the second xls file\n",
    "objects = df2['object'].values\n",
    "# A little cleanup to get rid of extra quotes\n",
    "print(objects[:3]) # before\n",
    "objects = np.array([o.strip(\"'''\") for o in objects])\n",
    "print(objects[:3]) # after (no extra quotes)\n",
    "conditions = df2['condition'].values\n",
    "# Similar cleanup\n",
    "conditions = np.array([c.strip(\"'\") for c in conditions])\n",
    "object_run = df2['run'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract one run of data to figure out how to parse the rest\n",
    "# Create a logical index for run 1\n",
    "ri = run==1 \n",
    "# (this index is over all rows in the xls file)\n",
    "print(ri.shape)\n",
    "# (73 of the 511 rows in the xls file are for run 1)\n",
    "print(ri.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for run 1 in the second xls file\n",
    "ri_objects = object_run==1\n",
    "objects_r1 = objects[ri_objects]\n",
    "conditions_r1 = conditions[ri_objects]\n",
    "# (there are 24 objects shown in run 1)\n",
    "print(len(objects_r1))\n",
    "print(len(conditions_r1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no category or variable stored that indicates TRIAL, so we have to get a bit tricky here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the category ()\n",
    "cat_r1 = cat[ri]\n",
    "# Select each entry that is NOT fixation and is NOT response\n",
    "fixation_r1 = cat_r1=='Fixation'\n",
    "response_r1 = cat_r1=='Response'\n",
    "# Word onsets were when it was NOT fixation and NOT \n",
    "word_on = ~(fixation_r1 | response_r1)\n",
    "# This leaves us with a logical index over rows for when the words were on, \n",
    "# which we can use to select rows to give us the onset times (in TR indices)\n",
    "# (this index is over rows in run 1 only - there are 73 rows relating to run 1 in the xls file)\n",
    "print(word_on.shape)\n",
    "# (there are 24 trials in run 1 - which is good, because it matches with the \n",
    "print(word_on.sum())\n",
    "# Select onset & offset values for run 1\n",
    "onsets_r1 = onset[ri]\n",
    "offsets_r1 = offset[ri]\n",
    "word_onsets_r1 = onsets_r1[word_on]\n",
    "word_offsets_r1 = offsets_r1[word_on]\n",
    "# This should match up with the xls row entries above.\n",
    "print('Words on:', word_onsets_r1)\n",
    "print('Words off:', word_offsets_r1)\n",
    "# Technically, we can ignore offsets, because all of these are 1-TR conditions.\n",
    "# Also, we have to set the indices to be zero-based, because python.\n",
    "word_onsets_r1 -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_TRs_per_run = 127\n",
    "\n",
    "# We can use these onsets to create a design matrix that we can use.\n",
    "# Here, we create a simple design matrix - one column, just ones at image onset.\n",
    "X_simple = np.zeros((n_TRs_per_run,1))\n",
    "for on in word_onsets_r1:\n",
    "    X_simple[on, 0] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_simple.T, aspect='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond2number = {'real':0, 'photo':1, 'foil':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond2number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond2number['foil']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we create a slightly more complex design matrix, with separate conditions for \n",
    "# real and image (with \"foil\" left out)\n",
    "n_conditions = 3\n",
    "X = np.zeros((n_TRs_per_run, n_conditions))\n",
    "# Define a dictionary to map the condition to a number\n",
    "cond2number = {'real':0, 'photo':1, 'foil':2}\n",
    "# \"enumerate\" returns an index (0, 1, 2, etc) along with the values in word_onsets_r1,\n",
    "# which we map to the \"itrial\" variable here\n",
    "for itrial, on in enumerate(word_onsets_r1):\n",
    "    print('---Trial %d---'%itrial) # simple formatting\n",
    "    this_condition = conditions_r1[itrial]\n",
    "    print(this_condition)\n",
    "    cond_idx = cond2number[this_condition]\n",
    "    print('... assigned to ', cond_idx)\n",
    "    X[on, cond_idx] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Et voila.\n",
    "plt.imshow(X.T, aspect='auto')\n",
    "plt.ylabel('Condition')\n",
    "plt.yticks([0, 1, 2], ['real', 'photo', 'foil'])\n",
    "plt.xlabel('Time (TRs)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real deal\n",
    "OK, so now our task is to do that for every run. THIS is the cell you should keep and modify when creating your own models. ***The main thing you will have to change is the dictionary that maps condition to a number.*** You will probably want to define a dict that is called `word2feature` or some such, which takes all the words in the experiment and maps them to one of several different features (which can be indicator variables, or whatever`*`). \n",
    "\n",
    "`*` there is an example of \"whatever\" below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with a list\n",
    "X_list = []\n",
    "n_runs = 7\n",
    "n_TRs_per_run = 127\n",
    "n_conditions = 3 # could be: n_features. If so don't forget to change it below.\n",
    "for this_run in range(1, n_runs+1):\n",
    "    Xtmp = np.zeros((n_TRs_per_run, n_conditions))\n",
    "    ri_xls1 = run==this_run\n",
    "    ri_xls2 = object_run==this_run\n",
    "    # -1 because python wants zero-based indices\n",
    "    # xls1 is too long - not just trials, but 3 values per trial. thus, fancier selection\n",
    "    # Word onsets were when it was NOT fixation and NOT \n",
    "    word_on = ~((cat[ri]=='Fixation') | (cat[ri]=='Response'))\n",
    "    all_onsets_thisrun = onset[ri_xls1]\n",
    "    word_onsets_thisrun = all_onsets_thisrun[word_on]\n",
    "    \n",
    "    conds_thisrun = conditions[ri_xls2]\n",
    "    objects_thisrun = conditions[ri_xls2]\n",
    "    # All of these variables should always be 24 long for this experiment\n",
    "    #print(len(objects_thisrun))\n",
    "    #print(len(conds_thisrun))\n",
    "    #print(len(word_onsets_thisrun))\n",
    "    for itrial in range(0,24):\n",
    "        on = word_onsets_thisrun[itrial]\n",
    "        o = objects_thisrun[itrial]\n",
    "        cond = conds_thisrun[itrial]\n",
    "        cond_idx = cond2number[cond]\n",
    "        # OR: define object2feature (see below), and call this:\n",
    "        #feature_idx = object2feature[o]\n",
    "        # ... and then use feature_idx as the index for Xtmp in the next line \n",
    "        # instead of cond_idx\n",
    "        Xtmp[on, cond_idx] = 1\n",
    "    # For each run, add the X variable we have created to a list:\n",
    "    X_list.append(Xtmp)\n",
    "# ... and concatenate everything here:\n",
    "X = np.vstack(X_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X.T, aspect='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way you could go about creating this X would be to create the full array of zeros (889 x 3) first, and then index into it. This would be a little more annoying, since all the trial indices you have start with 1 each run, and the trial indices into that big array for run 2 woudl have to start with 128. \n",
    "\n",
    "The way it's done above just keeps things a little simpler for bookkeeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Et voila.\n",
    "print(X.shape)\n",
    "plt.imshow(X.T, aspect='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define your own features, you will have to map words (or other variables from the excel file info) to your own conditions / features, using something like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new model like this: \n",
    "object2feature = {\n",
    "    'Head phones': 1,\n",
    "    'Ice cream scoop': 1,\n",
    "    'Bandages': 1,\n",
    "    'Ice scrapper': 1,\n",
    "    'Baseball glove': 1,\n",
    "    'Bow tie': 1,\n",
    "    'Camera': 1,\n",
    "    'Battery': 1,\n",
    "    'Carrots': 1,\n",
    "    'Beer mug': 1,\n",
    "    'Ladle': 1,\n",
    "    'Box knife': 1,\n",
    "    'Book': 1,\n",
    "    'Comb': 1,\n",
    "    'Lemon': 1,\n",
    "    'Apron': 1,\n",
    "    'Bird': 1,\n",
    "    'Acorn': 1,\n",
    "    'Lock': 1,\n",
    "    'Lollypop': 1,\n",
    "    'Can opener': 1,\n",
    "    'Banana': 1,\n",
    "    'Belt': 1,\n",
    "    'Magnifying glass': 1,\n",
    "    'Bird house': 1,\n",
    "    'Plate': 1,\n",
    "    'Golf ball': 1,\n",
    "    'Playing card': 1,\n",
    "    'Saucepan': 1,\n",
    "    'Razor': 1,\n",
    "    'Salt shaker': 1,\n",
    "    'Glass bottle': 1,\n",
    "    'Hammer': 1,\n",
    "    'Bowl': 1,\n",
    "    'Jack-O-Lantern': 1,\n",
    "    'Handbag': 1,\n",
    "    'Lint roller': 1,\n",
    "    'Bottle cap': 1,\n",
    "    'Scissors': 1,\n",
    "    'Bullet': 1,\n",
    "    'Bronze sponge': 1,\n",
    "    'Brick': 1,\n",
    "    'Bath sponge': 1,\n",
    "    'Lighter': 1,\n",
    "    'Binoculars': 1,\n",
    "    'Garden shovel': 1,\n",
    "    'Butter knife': 1,\n",
    "    'Bolt': 1,\n",
    "    'Coffee filter': 1,\n",
    "    'Dice': 1,\n",
    "    'Cork': 1,\n",
    "    'Dog': 1,\n",
    "    'Flip flop': 1,\n",
    "    'Dog bowl': 1,\n",
    "    'Corkscrew': 1,\n",
    "    'Extension cord': 1,\n",
    "    'Dishbrush': 1,\n",
    "    'Domino': 1,\n",
    "    'Flower': 1,\n",
    "    'Car lighter': 1,\n",
    "    'Dust pan': 1,\n",
    "    'Cactus': 1,\n",
    "    'Cotton balls': 1,\n",
    "    'Flashlight': 1,\n",
    "    'CD': 1,\n",
    "    'Cell phone': 1,\n",
    "    'Baby bottle': 1,\n",
    "    'Flask': 1,\n",
    "    'Fork': 1,\n",
    "    'Chalk': 1,\n",
    "    'Funnel': 1,\n",
    "    'Coat hook': 1,\n",
    "    'Dish soap': 1,\n",
    "    'Pine cone': 1,\n",
    "    'Turkey baster': 1,\n",
    "    'Sauce brush': 1,\n",
    "    'Scale': 1,\n",
    "    'Remote control': 1,\n",
    "    'Wine glass': 1,\n",
    "    'Game controller': 1,\n",
    "    'Vase': 1,\n",
    "    'Rubber duck': 1,\n",
    "    'Electrical tape': 1,\n",
    "    'Flower pot': 1,\n",
    "    'Door stop': 1,\n",
    "    'Tennis ball': 1,\n",
    "    'Curling Iron': 1,\n",
    "    'Pear': 1,\n",
    "    'Pizza cutter': 1,\n",
    "    'Swim goggles': 1,\n",
    "    'Crayon': 1,\n",
    "    'Door knob': 1,\n",
    "    'Light bulb': 1,\n",
    "    'Electrial outlet cover': 1,\n",
    "    'Hole punch': 1,\n",
    "    'Cow bell': 1,\n",
    "    'Frying pan': 1,\n",
    "    'Mouse': 1,\n",
    "    'Paint roller': 1,\n",
    "    'Pasta spoon': 1,\n",
    "    'Mason jar': 1,\n",
    "    'Matches': 1,\n",
    "    'Flyswatter': 1,\n",
    "    'Eye dropper': 1,\n",
    "    'Mug': 1,\n",
    "    'Gift bow': 1,\n",
    "    'Nail polish': 1,\n",
    "    'Paintbrush': 1,\n",
    "    'Pacifier': 1,\n",
    "    'Pencil': 1,\n",
    "    'Napkin holder': 1,\n",
    "    'Fuse': 1,\n",
    "    'Eye patch': 1,\n",
    "    'Nutcracker': 1,\n",
    "    'Picture frame': 1,\n",
    "    'Mitten': 1,\n",
    "    'Frisbee': 1,\n",
    "    'Piggy bank': 1,\n",
    "    'Plastic bottle': 1,\n",
    "    'Hair band': 1,\n",
    "    'Measuring cup': 1,\n",
    "    'Hand fan': 1,\n",
    "    'Hair clip': 1,\n",
    "    'Jewelry box': 1,\n",
    "    'Toy truck': 1,\n",
    "    'Toothbrush': 1,\n",
    "    'Tennis shoe': 1,\n",
    "    'High heel shoe': 1,\n",
    "    'Stapler': 1,\n",
    "    'Highlighter marker': 1,\n",
    "    'Thread': 1,\n",
    "    'Beanie': 1,\n",
    "    'Soap': 1,\n",
    "    'Ice tray': 1,\n",
    "    'Tape': 1,\n",
    "    'Dumbbell': 1,\n",
    "    'Sponge': 1,\n",
    "    'Tea bag': 1,\n",
    "    'Grater': 1,\n",
    "    'Timer': 1,\n",
    "    'Hour glass': 1,\n",
    "    'Tongs': 1,\n",
    "    'Spatula': 1,\n",
    "    'Handsaw': 1,\n",
    "    'Ashtray': 1,\n",
    "    'Basket': 1,\n",
    "    'Egg slicer': 1,\n",
    "    'Shot glass': 1,\n",
    "    'Medicine bottle': 1,\n",
    "    'Bell': 1,\n",
    "    'Birdie': 1,\n",
    "    'Oven mitt': 1,\n",
    "    'Candle': 1,\n",
    "    'Whistle': 1,\n",
    "    'Wrench': 1,\n",
    "    'Checkers': 1,\n",
    "    'Phone': 1,\n",
    "    'Drink shaker': 1,\n",
    "    'Clothes hanger': 1,\n",
    "    'Clothes pin': 1,\n",
    "    'Straw': 1,\n",
    "    'Wire cutters': 1,\n",
    "    'Lipstick': 1,\n",
    "    'MP3 Player': 1,\n",
    "    'Snow goggles': 1,\n",
    "    'Measuring tape': 1,\n",
    "    'Hand blender': 1,\n",
    "    'Butter dish': 1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load fMRI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdir = '/unrshare/LESCROARTSHARE/data_PSY763/SnowLabData/'\n",
    "all_files = sorted(glob.glob(fdir + '*mat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsmoothed_files = all_files[::2]\n",
    "smoothed_files = all_files[1::2]\n",
    "# Show what we've done with this indexing:\n",
    "for f in unsmoothed_files:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list for data\n",
    "data = []\n",
    "# Load each file into list. NOTE that here we are choosing smoothed or unsmoothed data!\n",
    "for file in smoothed_files:\n",
    "    with h5py.File(file) as hf:\n",
    "        d = hf['data'].value\n",
    "    print('Original size: ', d.shape)\n",
    "    # Transpose data so time is first axis\n",
    "    d = d.T\n",
    "    # Map the 4 values returned by d.shape to separate variables\n",
    "    t, z, y, x = d.shape\n",
    "    print('Transposed size: ', d.shape)\n",
    "    # Reshape data to be time x (all voxels)\n",
    "    d = np.reshape(d, (127, -1)) # the -1 here means string everything all out into one vector\n",
    "    print('Reshaped size: ', d.shape)\n",
    "    # standardize by run, because that makes many things easier\n",
    "    data.append(zscore(d, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time is now first dimension; stack everything up\n",
    "Y = np.vstack(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check it out: X and Y.\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to account for the HRF in your X! See the functions in utils.fmri for a useful utility function.\n",
    "\n",
    "Other utility functions you will need are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "imp.reload(utils.fmri)\n",
    "imp.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a makeshif 3D brain on which to plot your data\n",
    "brain = utils.fmri.get_brain(unsmoothed_files[0])\n",
    "print(brain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an arbitrary ROI\n",
    "roi = np.zeros_like(brain)\n",
    "roi[35:45, 5:15, 8:-8] = 1\n",
    "# Flatten, so this will be like other statistical results derived\n",
    "# from your X / Y matrices:\n",
    "roi_flat = roi.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the utility function to dispay the ROI as you would data. \n",
    "\n",
    "### Some notes on the function utils.fmri.overlay_brain():\n",
    "The `threshold` input crops out the zeros in the ROI (if set above zero); if you don't set threshold, you won't see the brain underneath the data at all. Same applies to your results. \n",
    "\n",
    "**WARNING**: NaNs in your data will mess up image plots (and many others, too). NaNs can come from dividing by zero (e.g. for voxels outside the brain). Shit happens. You can convert nans to zeros using the function `np.nan_to_num`\n",
    "\n",
    "You also want to play with the vmin / vmax arguments to this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the ROI\n",
    "utils.fmri.overlay_brain(roi_flat, brain, threshold=0.5, cmap='inferno',\n",
    "                  vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: to use `roi` (or `roi_flat`, for that matter) as a logical index, you have to convert the values in it to True / False values instead of 1s and 0s. How would you do this...? (There are examples in the class notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
