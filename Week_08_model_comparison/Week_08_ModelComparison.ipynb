{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model comparison\n",
    "This week we will introduce two main concepts: noise ceiling estimation and variance partitioning. We will blaze by a few difficult concepts that will be addressed in greater depth in an auxiliary notebook, which you should run through over spring break. These auxiliary notebooks will contain many more basic python exercises, which are meant to help you get more comfortable with the idioms of python code that are often used in this class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# For visualization of data\n",
    "import cortex as cx\n",
    "# For loading hdf5 files in python\n",
    "import h5py\n",
    "# Some utility functions in a file in this directory:\n",
    "import utils\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored in hdf format, which is a generic data storage format that is compatible with both matlab and python (using the h5py or tables libraries). You can read more about the file specification on the [hdf5 support page](https://portal.hdfgroup.org/display/HDF5/HDF5) or the [wikipedia page about hdf format](https://en.wikipedia.org/wiki/Hierarchical_Data_Format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files we will be inspecting are all in `/unrshare/LESCROARTSHARE/IntroToEncodingModels/`. Let's see what's there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls /unrshare/LESCROARTSHARE/IntroToEncodingModels/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File storing fMRI data for one subject\n",
    "exp_file = '/unrshare/LESCROARTSHARE/IntroToEncodingModels/s01_color_natims_data.hdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the arrays stored in this file?\n",
    "with h5py.File(exp_file, mode='r') as df:\n",
    "    for k in df.keys():\n",
    "        print(k, df[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "with h5py.File(exp_file) as hf:\n",
    "    Y_est = hf['est'].value\n",
    "    Y_val = hf['val'].value\n",
    "    Y_val_rpts = hf['val_rpts'].value\n",
    "    #Y_rpts = hf['val_rpts'].value\n",
    "    mask = hf['mask'].value > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is this \"mask\" array?\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A mask is a bunch of true and false values that allow you to select values from another array. True values are selected, False values are not. Thus, masks provide a useful way to clip out the fMRI data you care about from the data you don't. (For example, you might not want to analyze voxels which are outside your area of interest, noisy, or entirely outside the brain). \n",
    "\n",
    "A mask should be spaitally organized in the same volumetric format as the data collected in an experiment (or whatever shape that data has been resampled into). Masks in FSL, Brain Voyager, SPM, Freesurfer, and other packages all work this way. This is just showing you what happens under the hood. So, let's see the mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the mask\n",
    "fig = utils.slice_3d_array(mask, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other notebook will have a lot more to say about masking; for now, we will just walk through a few examples of how to unmask the data in the stored arrays (i.e., how to convert that data from an array of (time x voxels) to an array in the shape of a brain (time x Z x Y x X). \n",
    "\n",
    "For our first exmaple, just for funzies, let's visualize the estimated responses to one stimulus in the estimation data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_est[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 's01'\n",
    "transform = 'color_natims' \n",
    "vol = cx.Volume(Y_est[0], subject, transform, vmin=-3, vmax=3, cmap='RdBu_r', mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show responses in 2D flatmap with \"quickflat\" module\n",
    "fig = cx.quickflat.make_figure(vol, with_curvature=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show responses in 3D in webgl viewer in your browser\n",
    "h = cx.webgl.show(vol, open_browser=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise ceiling\n",
    "OK! now on to noise ceilings! The simplest way to compute a noise ceiling is to simply correlate the responses to each image in the validation data. The first dimension of Y_val_rpts is the repeat dimension (there are 11 different estimated responses to all images). \n",
    "\n",
    "## Exercise \n",
    "Select each repeat in in turn, and correlate the responses across images with the other repeats using `utils.column_corr()`. There are many possible combinations of repeats! A great way to list all those combinations is with a library called `itertools`; check out the `itertools.combinations()` function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as itools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NICE!\n",
    "combos = itools.combinations(np.arange(3), 2)\n",
    "list(combos)\n",
    "# How can you use this in your answer??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise \n",
    "Show the noise ceiling on the brain! Use pycortex (`cx.Volume()`, and `cx.quickflat.make_figure()` or `cx.webgl.show()`) to display the data vector you have on the brain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus exercise (we won't stop for this, but see if you can do it)\n",
    "Put the estimated noise ceiling (which exists as a vector of values) back into a 3D volume, and display the image in slices as was done with the mask above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making predictions of the validation data\n",
    "To fit the models and make predictions of the validation data, we need our design matrices for each invidivdual model. These are stored in `color_natims_features_<model>.hdf` files. You can load them as we loaded the data above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_base = '/unrshare/LESCROARTSHARE/IntroToEncodingModels/color_natims_features_%s.hdf'\n",
    "with h5py.File(fname_base%'19cat') as hf:\n",
    "    Xsem_est = hf['est'].value\n",
    "    Xsem_val = hf['val'].value\n",
    "\n",
    "with h5py.File(fname_base%'fft') as hf:\n",
    "    Xfft_est = hf['est'].value\n",
    "    Xfft_val = hf['val'].value\n",
    "\n",
    "with h5py.File(fname_base%'dst') as hf:\n",
    "    Xdst_est = hf['est'].value\n",
    "    Xdst_val = hf['val'].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check on some shapes of arrays to see if this is all sensible:\n",
    "Xdst_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this is pretty easy from here, since we don't have to worry about the HRF - the estimated responses for each image area already based on multiple presentations of the image, so the HRF has already been factored in. Now, we just do a regression of these feature values onto the Y values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple regresion function\n",
    "def simple_ols(X, Y):\n",
    "    return np.linalg.pinv(X.T.dot(X)).dot(X.T.dot(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with the smallest model, the distance model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate beta weights\n",
    "Bdst = simple_ols(Xdst_est, Y_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what shape you get out. Is this sensible?\n",
    "Bdst.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "Plot the differences in beta values between the nearest distance and the farthest distance across the brain! This will make something like Figure 6A from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict the validation responses based on the distance model\n",
    "Yhat_dst = Xdst_val.dot(Bdst)\n",
    "r_dst = utils.column_corr(Yhat_dst, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of nasty nans\n",
    "r_dst[np.isnan(r_dst)] = 0\n",
    "# And plot a histogram of prediction accuracy\n",
    "_ = plt.hist(r_dst, bins=31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise \n",
    "Show prediction accuracy in the brain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Compute r_sem and r_fft, the prediction accuracy for the other two models. Also, compute r_dst_sem, the prediction accuracy for a COMBINED model of distance and semantic selectivity. To create the X variable that you will use in regression for these models, simply concatenate the X variables for the distance and semantic feature spaces in the features dimension, and re-fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance partitioning \n",
    "For simplicity, we will only do variace partitioning between two models: the distance and semantic feature spaces. We will ultimately partly re-create part of figure 8 from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lazy computation of r2\n",
    "r2_dst = np.sign(r_dst) * r_dst**2\n",
    "r2_sem = np.sign(r_sem) * r_sem**2\n",
    "r2_dst_sem = np.sign(r_dst_sem) * r_dst_sem**2\n",
    "\n",
    "# Variance partitioning\n",
    "r2_unique_dst = r2_dst_sem - r2_sem\n",
    "r2_unique_sem = r2_dst_sem - r2_dst\n",
    "r2_shared = r2_dst_sem - r2_unique_dst - r2_unique_sem\n",
    "\n",
    "# Show shared variance\n",
    "fig1 = cx.quickflat.make_figure(cx.Volume(r2_shared, subject, transform, cmap='hot', mask=mask, vmin=0, vmax=0.2), with_curvature=True)\n",
    "fig1.gca().set_title('Shared variance')\n",
    "# Show unique variance for the semantic model\n",
    "fig2 = cx.quickflat.make_figure(cx.Volume(r2_unique_sem, subject, transform, cmap='hot', mask=mask, vmin=0, vmax=0.2), with_curvature=True)\n",
    "fig2.gca().set_title('Unique semantic variance')\n",
    "# Show unique variance for the distance model\n",
    "fig3 = cx.quickflat.make_figure(cx.Volume(r2_unique_dst, subject, transform, cmap='hot', mask=mask, vmin=0, vmax=0.2), with_curvature=True)\n",
    "fig3.gca().set_title('Unique distance variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Make ROI plots\n",
    "... Will be done in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "roi_m = cx.get_roi_masks(subject, transform, roi_list=['V1','FFA','PPA','RSC','OPA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
