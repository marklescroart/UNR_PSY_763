{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating $\\beta$ weights: OLS vs Ridge\n",
    "This week we will discuss the difference between Ordinary Least Squares (OLS) regression and Ridge regression as ways to estimate model parameters ($\\beta$ weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from stat_utils import column_corr\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will create some fake data, estimate responses to it, and predict responses in a new data set.\n",
    "\n",
    "For now, for simplicity, we will not worry about the HRF, and we will consider a more general design that does not involve a design matrix of ones and zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 01: Make some fake data!\n",
    "* Create a design matrix with 100 different channels in it, and with 290 time points (TRs, if that's clearer.)\n",
    "* Generate 50 random weights for each column for each of 10 voxels\n",
    "* Generate data timecourses (Y variables) for all 10 voxels (each with a different set of $\\beta$ weights)\n",
    "* Split the design matrix and data into training and validation sets by taking 200 time points for training and 90 time points for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "# Parameters\n",
    "n_wts = 50\n",
    "n_tps_trn = 200\n",
    "n_tps_val = 90\n",
    "n_vox = 10 \n",
    "noise_magnitude = 10\n",
    "\n",
    "X = np.random.randn(n_tps_trn + n_tps_val, n_wts)\n",
    "B = np.random.randn(n_wts, n_vox)\n",
    "# Some noise! Otherwise it's no fun.\n",
    "E = np.random.randn(n_tps_trn + n_tps_val, n_vox) * noise_magnitude\n",
    "Y = X.dot(B) + E\n",
    "Xtrn = X[:n_tps_trn, :]\n",
    "Xval = X[n_tps_trn:, :]\n",
    "Ytrn = Y[:n_tps_trn, :]\n",
    "Yval = Y[n_tps_trn:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 02: Put the OLS function into python and estimate your weights!\n",
    "\n",
    "The normal equation for OLS is: \n",
    "\n",
    "## $\\beta = (X^TX)^{-1}X^TY$\n",
    "\n",
    "Define a function: \n",
    "\n",
    "```python\n",
    "def ols(X, Y): \n",
    "    B = ....\n",
    "    return B\n",
    "```\n",
    "to do OLS estimation of weights for you!\n",
    "\n",
    "Hint: to do matrix inversion, use `np.linalg.inv()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Answer\n",
    "def ols(X, Y):\n",
    "    B = np.linalg.inv(X.T.dot(X)).dot(X.T.dot(Y))\n",
    "    return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how well OLS estimates $\\beta$ weights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot estimated beta weights against true beta weights\n",
    "# We will use this several times, so let's define a function:\n",
    "def plot_beta_comparison(B, Be):\n",
    "    fig, axs = plt.subplots(5, 2, figsize=(8,6))\n",
    "    # Use only training data to estimate weights\n",
    "    B_est = ols(Xtrn, Ytrn)\n",
    "    for b, be, ax in zip(B.T, Be.T, axs.flatten()):\n",
    "        ax.plot(b)\n",
    "        ax.plot(be, 'r.')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_beta_comparison(B, B_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions\n",
    "\n",
    "Compute predictions by multiplying the design matrix for the validation data (`Xval`) by the estimated weights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred = Xval.dot(B_est)\n",
    "Y_pred.shape\n",
    "r = column_corr(Yval, Y_pred)\n",
    "plt.plot(r, 'o')\n",
    "plt.ylim([0, 1])\n",
    "# Not bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How can we mess this up? \n",
    "A simple way is to add noise! Amp up the noise above and see what happens to the estimates of the $\\beta$ weights and to the predictions.\n",
    "\n",
    "[Go do it!]\n",
    "\n",
    "Another way to mess up the estimation of regressors is to add correlations between regressors. This is a particular problem if the correlation between your regressors is different in the training data and in the validation data (i.e., if your training data is not representative of the real world). Let's simulate this situation by creating an `Xtrn` matrix that has correlated columns (while `Xval` does not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 03: Make a design matrix with correlated columns!\n",
    "\n",
    "Call it Xc_trn (for \"X correlated\")\n",
    "\n",
    "use `np.corrcoef` to compute the correlations between columns to see if you've succeeded!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Answer\n",
    "n_wts = 50\n",
    "ntot = n_tps_trn + n_tps_val\n",
    "X0 = np.random.randn(n_tps_trn, 1)\n",
    "corr_disruption = 0.3\n",
    "n_correlated = 10\n",
    "Xc_trn = np.hstack([np.random.randn(n_tps_trn, 1) * corr_disruption + X0 for q in range(n_correlated)])\n",
    "Xc_trn = np.hstack([Xc_trn, np.random.randn(n_tps_trn, n_wts-n_correlated)])\n",
    "\n",
    "# Demonstrate what structure looks like\n",
    "plt.imshow(Xc_trn, aspect='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test whether you have succeeded \n",
    "plt.imshow(np.corrcoef(Xc_trn.T), vmin=-1, vmax=1, cmap='RdBu_r')\n",
    "plt.colorbar();\n",
    "# The upper left corner of this plot should be red, indicating correlated columns of Xc_trn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now, re-generate your training and validation data from this correlated Xc\n",
    "\n",
    "Yc_trn = Xc_trn.dot(B) + E[:n_tps_trn, :] #  Use same E as above\n",
    "\n",
    "Xc_val = np.random.randn(n_tps_val, n_wts)\n",
    "Yc_val = Xc_val.dot(B) + E[n_tps_trn:, :] #  Use same E as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "B_est_c = ols(Xc_trn, Yc_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_beta_comparison(B, B_est_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "Y_pred_c = Xc_val.dot(B_est_c)\n",
    "rc = column_corr(Yc_val, Y_pred_c)\n",
    "plt.plot(r, 'ro', label='Un-correlated columns')\n",
    "plt.plot(rc, 'bo', label='Correlated columns')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "# Messing up a little more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 05: Implement ridge regression \n",
    "\n",
    "The normal equation for ridge regression is: \n",
    "    \n",
    "## $\\beta = (X^TX + \\lambda I)^{-1}X^TY$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xc_trn.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xc_trn.T.dot(Xc_trn).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Answer\n",
    "def ridge(X, Y, lam=100):\n",
    "    nt = X.shape[1]\n",
    "    B = np.linalg.inv(X.T.dot(X) + lam * np.eye(nt)).dot(X.T.dot(Y))\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "B_est_c_ridge = ridge(Xc_trn, Yc_trn, lam=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The weights are smaller!\n",
    "plot_beta_comparison(B, B_est_c_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "Y_pred_c_ridge = Xc_val.dot(B_est_c_ridge)\n",
    "rc_ridge = column_corr(Yc_val, Y_pred_c_ridge)\n",
    "plt.plot(r, 'ro', label='Un-correlated columns')\n",
    "plt.plot(rc, 'bo', label='Correlated columns')\n",
    "plt.plot(rc_ridge, 'g*', label='Correlated columns, ridge')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "# Messing up a little more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In *MOST* Cases, this improves your prediction accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: How would you go about choosing a lambda parameter? \n",
    "\n",
    "\n",
    "Try different ones! See which works best! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Answer\n",
    "```\n",
    "\n",
    "\n",
    "Write out the answer to this exercise in pseudo-code!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If time: More demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ..."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
